{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809bd3e4-3b3e-49e3-b0b6-8cee9d41488c",
   "metadata": {},
   "source": [
    "## 0. Understanding the Problem\n",
    "\n",
    "The process of classifying the authors for a given text is called authorship attribution. Each author writes about different topics and has their own style of writing (author fingerprint) which allows for the identification. Applications of this kind of model include plaigarism detection and resolving the disputed authorship. \n",
    "\n",
    "In the dataset given there are 2 columns: Author and Text\n",
    "This makes it a supervised learning problem since there is data and a assigned label  to each text. \n",
    "\n",
    "In the problem, the cost of false positive and false negatives both carry significant consequences. Therefore, a good model should have a balance of sensitivity and specificity. F1-score would be the ideal metric for the model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588fb9a-e0cc-4b0b-bd01-d13ab15cdda1",
   "metadata": {},
   "source": [
    "## 1. Installing and Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e58bcfe-ea5f-42a4-88f1-c91208637056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install openpyxl --upgrade\n",
    "# %pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12f2bac3-d979-42f9-b5d0-5f8ae379e832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams, trigrams, FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import textstat\n",
    "import warnings\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style(\"darkgrid\")\n",
    "palette = \"cool\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d742f4a-1ea3-4b82-89cb-f5deb6611c6d",
   "metadata": {},
   "source": [
    "## DATA UNDERSTANDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08383193-451b-4d05-9504-138d0ca8012d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scoring in PROC DISCRIM is as easy as validati...</td>\n",
       "      <td>AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the GLM procedure, you may have used LSMEAN...</td>\n",
       "      <td>AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first problem, accuracy of the data file, ...</td>\n",
       "      <td>AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the homogeneity of covariance matrices assu...</td>\n",
       "      <td>AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With a CONTRAST statement, you specify L, in t...</td>\n",
       "      <td>AM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author\n",
       "0  Scoring in PROC DISCRIM is as easy as validati...     AM\n",
       "1  In the GLM procedure, you may have used LSMEAN...     AM\n",
       "2  The first problem, accuracy of the data file, ...     AM\n",
       "3  If the homogeneity of covariance matrices assu...     AM\n",
       "4  With a CONTRAST statement, you specify L, in t...     AM"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_excel(\"Assignment_Data/Data.xlsx\")\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ed65f0-c620-43b8-b252-860f19945e56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1922, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "886e5335-99e4-4fbb-a5fc-a3e039773bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1922 entries, 0 to 1921\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    1922 non-null   object\n",
      " 1   Author  1922 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 30.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()\n",
    "# no null values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "448bf9dc-2439-441e-88f8-d1b8790fe151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1106 duplicate rows are present within the data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>%distribution(data=&amp;data,out=&amp;report_name,cont...</td>\n",
       "      <td>DM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>%distribution(data=&amp;data,out=&amp;report_name,cont...</td>\n",
       "      <td>DM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>%generate_grouping(from=work.profile_codes,val...</td>\n",
       "      <td>DM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>%generate_grouping(from=work.profile_codes,val...</td>\n",
       "      <td>DM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>*\\tTemporal infidelity occurs when model input...</td>\n",
       "      <td>DM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>*\\tTemporal infidelity occurs when model input...</td>\n",
       "      <td>DM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>*\\texamining group differences on predictor va...</td>\n",
       "      <td>AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>*\\texamining group differences on predictor va...</td>\n",
       "      <td>AM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text Author\n",
       "1367  %distribution(data=&data,out=&report_name,cont...     DM\n",
       "1419  %distribution(data=&data,out=&report_name,cont...     DM\n",
       "1414  %generate_grouping(from=work.profile_codes,val...     DM\n",
       "1243  %generate_grouping(from=work.profile_codes,val...     DM\n",
       "1362  *\\tTemporal infidelity occurs when model input...     DM\n",
       "1370  *\\tTemporal infidelity occurs when model input...     DM\n",
       "248   *\\texamining group differences on predictor va...     AM\n",
       "44    *\\texamining group differences on predictor va...     AM"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check for duplicates in the text data \n",
    "duplicate_data = df_train.duplicated(keep=\"first\")\n",
    "\n",
    "print(duplicate_data.sum(), \"duplicate rows are present within the data\")\n",
    "\n",
    "\n",
    "display(df_train[duplicate_data].sort_values(by=\"Text\").head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e842aff-b004-4102-8d6a-712ce8ba2980",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(816, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove the duplicate rows within the dataset\n",
    "df_train.drop_duplicates(inplace=True)\n",
    "display(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e7967-010e-439b-8780-94d72942f70a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    There's quite a lot of duplicated rows present. These will be unhelpful for training the model and need to be removed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "409c1ea5-aee8-450d-8e7b-b7ecaa09261c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scoring in PROC DISCRIM is as easy as validati...</td>\n",
       "      <td>AM</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the GLM procedure, you may have used LSMEAN...</td>\n",
       "      <td>AM</td>\n",
       "      <td>782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first problem, accuracy of the data file, ...</td>\n",
       "      <td>AM</td>\n",
       "      <td>990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the homogeneity of covariance matrices assu...</td>\n",
       "      <td>AM</td>\n",
       "      <td>934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With a CONTRAST statement, you specify L, in t...</td>\n",
       "      <td>AM</td>\n",
       "      <td>1490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author  sentence_length\n",
       "0  Scoring in PROC DISCRIM is as easy as validati...     AM              215\n",
       "1  In the GLM procedure, you may have used LSMEAN...     AM              782\n",
       "2  The first problem, accuracy of the data file, ...     AM              990\n",
       "3  If the homogeneity of covariance matrices assu...     AM              934\n",
       "4  With a CONTRAST statement, you specify L, in t...     AM             1490"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"sentence_length\"] = df_train.Text.apply(len)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b3ca461-e8ec-46e6-a07b-ef893fe3144c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     816.000000\n",
       "mean      688.286765\n",
       "std       532.106926\n",
       "min        61.000000\n",
       "25%       304.000000\n",
       "50%       526.500000\n",
       "75%       922.000000\n",
       "max      4096.000000\n",
       "Name: sentence_length, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"sentence_length\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2e5e0e-0f6d-4429-b977-f8bcd30ef238",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    --> The smallest text is 61 characters long and the longest text is 4096 characters long.<br>\n",
    "    --> The standard deviation is 547 characters which shows that the lengths are not uniform and that they vary widely.<br>\n",
    "    --> Mean is greater than median, this means that it is a right skewed distribution with majority of the texts are short and it is the longer texts which are pulling the mean up, indicating the presence of outliers.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab78b788-a171-4503-b774-95be87b88ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scoring in PROC DISCRIM is as easy as validati...</td>\n",
       "      <td>AM</td>\n",
       "      <td>215</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the GLM procedure, you may have used LSMEAN...</td>\n",
       "      <td>AM</td>\n",
       "      <td>782</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first problem, accuracy of the data file, ...</td>\n",
       "      <td>AM</td>\n",
       "      <td>990</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the homogeneity of covariance matrices assu...</td>\n",
       "      <td>AM</td>\n",
       "      <td>934</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With a CONTRAST statement, you specify L, in t...</td>\n",
       "      <td>AM</td>\n",
       "      <td>1490</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author  sentence_length  \\\n",
       "0  Scoring in PROC DISCRIM is as easy as validati...     AM              215   \n",
       "1  In the GLM procedure, you may have used LSMEAN...     AM              782   \n",
       "2  The first problem, accuracy of the data file, ...     AM              990   \n",
       "3  If the homogeneity of covariance matrices assu...     AM              934   \n",
       "4  With a CONTRAST statement, you specify L, in t...     AM             1490   \n",
       "\n",
       "   word_count  \n",
       "0          37  \n",
       "1         129  \n",
       "2         159  \n",
       "3         146  \n",
       "4         247  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to count the number of words inside a sentence\n",
    "def word_counter(sent):\n",
    "    return len(sent.split(\" \"))\n",
    "\n",
    "df_train[\"word_count\"] = df_train[\"Text\"].apply(word_counter)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a1b3a78-bd46-409a-a025-6b969c83962d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scoring in PROC DISCRIM is as easy as validati...</td>\n",
       "      <td>AM</td>\n",
       "      <td>215</td>\n",
       "      <td>37</td>\n",
       "      <td>5.810811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the GLM procedure, you may have used LSMEAN...</td>\n",
       "      <td>AM</td>\n",
       "      <td>782</td>\n",
       "      <td>129</td>\n",
       "      <td>6.062016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first problem, accuracy of the data file, ...</td>\n",
       "      <td>AM</td>\n",
       "      <td>990</td>\n",
       "      <td>159</td>\n",
       "      <td>6.226415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the homogeneity of covariance matrices assu...</td>\n",
       "      <td>AM</td>\n",
       "      <td>934</td>\n",
       "      <td>146</td>\n",
       "      <td>6.397260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With a CONTRAST statement, you specify L, in t...</td>\n",
       "      <td>AM</td>\n",
       "      <td>1490</td>\n",
       "      <td>247</td>\n",
       "      <td>6.032389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author  sentence_length  \\\n",
       "0  Scoring in PROC DISCRIM is as easy as validati...     AM              215   \n",
       "1  In the GLM procedure, you may have used LSMEAN...     AM              782   \n",
       "2  The first problem, accuracy of the data file, ...     AM              990   \n",
       "3  If the homogeneity of covariance matrices assu...     AM              934   \n",
       "4  With a CONTRAST statement, you specify L, in t...     AM             1490   \n",
       "\n",
       "   word_count  avg_word_length  \n",
       "0          37         5.810811  \n",
       "1         129         6.062016  \n",
       "2         159         6.226415  \n",
       "3         146         6.397260  \n",
       "4         247         6.032389  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the average word length\n",
    "\n",
    "def avg_word_length(sent): \n",
    "    sent_len = len(sent.split(\" \"))\n",
    "    return sum([len(wrd)  for wrd in sent]) / sent_len\n",
    "\n",
    "df_train[\"avg_word_length\"] = df_train[\"Text\"].apply(avg_word_length)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1b1371d-01ae-4568-a874-d9f95fc50f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word_count</th>\n",
       "      <td>816.0</td>\n",
       "      <td>108.006127</td>\n",
       "      <td>84.159438</td>\n",
       "      <td>12.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>704.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_word_length</th>\n",
       "      <td>816.0</td>\n",
       "      <td>6.429311</td>\n",
       "      <td>0.597206</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.056834</td>\n",
       "      <td>6.349312</td>\n",
       "      <td>6.710606</td>\n",
       "      <td>10.9375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 count        mean        std   min        25%        50%  \\\n",
       "word_count       816.0  108.006127  84.159438  12.0  47.000000  81.000000   \n",
       "avg_word_length  816.0    6.429311   0.597206   5.0   6.056834   6.349312   \n",
       "\n",
       "                        75%       max  \n",
       "word_count       147.000000  704.0000  \n",
       "avg_word_length    6.710606   10.9375  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[[\"word_count\", \"avg_word_length\"]].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "533109f0-7a83-4aba-bc4d-db596629ead0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Author\n",
       "AM    1.0\n",
       "CD    1.0\n",
       "DM    1.0\n",
       "DO    1.0\n",
       "FE    1.0\n",
       "TK    1.0\n",
       "Name: Lexical_Diversity, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding a type-token ratio feature to check the lexical richness of the text, it is measured by looking at the ratio of the unique tokens to the total number of tokens used within the text. \n",
    "\n",
    "# the aim is to look at the lexical richness of each author's texts by grouping by the author name.\n",
    "\n",
    "\n",
    "def text_tokenizer(sent):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sent = sent.lower() # convert the text to lowercase\n",
    "    tokens = re.split(r'\\W+', sent) # split text based on non word characters\n",
    "    clean_tokens = [lemmatizer.lemmatize(i) for i in tokens if i not in string.punctuation and not i.isdigit()] # removing number, punctuation and lemmatizing the text\n",
    "    return set(clean_tokens) # return set of clean tokens\n",
    "    \n",
    "    \n",
    "def lexical_diversity(text):\n",
    "    tokens = text_tokenizer(text)\n",
    "    types = len(set(tokens))\n",
    "    return types / len(tokens) if tokens else 0\n",
    "\n",
    "\n",
    "df_train['Lexical_Diversity'] = df_train['Text'].apply(lexical_diversity)\n",
    "author_lexical_diversity = df_train.groupby('Author')['Lexical_Diversity'].mean()\n",
    "\n",
    "\n",
    "# Display the results\n",
    "author_lexical_diversity.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e21600c3-9ba0-47ce-8ec1-2f25c687dad3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'type_to_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/torchenv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'type_to_token'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# looking at the row with the smallest type to token ratio. \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m small_ttt \u001b[38;5;241m=\u001b[39m \u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype_to_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39margmin()\n\u001b[1;32m      3\u001b[0m df_train\u001b[38;5;241m.\u001b[39miloc[small_ttt]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torchenv/lib/python3.9/site-packages/pandas/core/frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3896\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3898\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torchenv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'type_to_token'"
     ]
    }
   ],
   "source": [
    "# looking at the row with the smallest type to token ratio. \n",
    "small_ttt = df_train[\"type_to_token\"].argmin()\n",
    "df_train.iloc[small_ttt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3466b66-d8fb-4c15-b7d5-1a23245e2f16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>type_to_token_by_author</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>comma_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scoring in PROC DISCRIM is as easy as validati...</td>\n",
       "      <td>AM</td>\n",
       "      <td>215</td>\n",
       "      <td>37</td>\n",
       "      <td>5.810811</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the GLM procedure, you may have used LSMEAN...</td>\n",
       "      <td>AM</td>\n",
       "      <td>782</td>\n",
       "      <td>129</td>\n",
       "      <td>6.062016</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first problem, accuracy of the data file, ...</td>\n",
       "      <td>AM</td>\n",
       "      <td>990</td>\n",
       "      <td>159</td>\n",
       "      <td>6.226415</td>\n",
       "      <td>0.583851</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the homogeneity of covariance matrices assu...</td>\n",
       "      <td>AM</td>\n",
       "      <td>934</td>\n",
       "      <td>146</td>\n",
       "      <td>6.397260</td>\n",
       "      <td>0.463087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With a CONTRAST statement, you specify L, in t...</td>\n",
       "      <td>AM</td>\n",
       "      <td>1490</td>\n",
       "      <td>247</td>\n",
       "      <td>6.032389</td>\n",
       "      <td>0.370518</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author  sentence_length  \\\n",
       "0  Scoring in PROC DISCRIM is as easy as validati...     AM              215   \n",
       "1  In the GLM procedure, you may have used LSMEAN...     AM              782   \n",
       "2  The first problem, accuracy of the data file, ...     AM              990   \n",
       "3  If the homogeneity of covariance matrices assu...     AM              934   \n",
       "4  With a CONTRAST statement, you specify L, in t...     AM             1490   \n",
       "\n",
       "   word_count  avg_word_length  type_to_token_by_author  Lexical_Diversity  \\\n",
       "0          37         5.810811                 0.756757                1.0   \n",
       "1         129         6.062016                 0.542636                1.0   \n",
       "2         159         6.226415                 0.583851                1.0   \n",
       "3         146         6.397260                 0.463087                1.0   \n",
       "4         247         6.032389                 0.370518                1.0   \n",
       "\n",
       "   comma_count  \n",
       "0            0  \n",
       "1            4  \n",
       "2            6  \n",
       "3            4  \n",
       "4            8  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"comma_count\"] = df_train[\"Text\"].str.count(\",\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb2e0be2-d043-4775-b7d3-04e6a3872ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def avg_sentence_length(txt):\n",
    "    sents = re.split(r'[.!?]+', txt)\n",
    "    sents = [sent.strip() for sent in sents if sent.strip()]\n",
    "    word_counts = [len(sent.split()) for sent in sents]\n",
    "\n",
    "    if len(word_counts) > 0:\n",
    "        return sum(word_counts) / len(word_counts)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df_train['avg_sentence_length'] = df_train['Text'].apply(avg_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bda80e8-c5c6-4e7f-a03d-56a57062fa90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>type_to_token_by_author</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>comma_count</th>\n",
       "      <th>Avg_Sentence_Length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scoring in PROC DISCRIM is as easy as validati...</td>\n",
       "      <td>AM</td>\n",
       "      <td>215</td>\n",
       "      <td>37</td>\n",
       "      <td>5.810811</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>12.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the GLM procedure, you may have used LSMEAN...</td>\n",
       "      <td>AM</td>\n",
       "      <td>782</td>\n",
       "      <td>129</td>\n",
       "      <td>6.062016</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>18.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first problem, accuracy of the data file, ...</td>\n",
       "      <td>AM</td>\n",
       "      <td>990</td>\n",
       "      <td>159</td>\n",
       "      <td>6.226415</td>\n",
       "      <td>0.583851</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the homogeneity of covariance matrices assu...</td>\n",
       "      <td>AM</td>\n",
       "      <td>934</td>\n",
       "      <td>146</td>\n",
       "      <td>6.397260</td>\n",
       "      <td>0.463087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>18.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With a CONTRAST statement, you specify L, in t...</td>\n",
       "      <td>AM</td>\n",
       "      <td>1490</td>\n",
       "      <td>247</td>\n",
       "      <td>6.032389</td>\n",
       "      <td>0.370518</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>15.687500</td>\n",
       "      <td>15.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>6. Almost everyone will agree that we live in ...</td>\n",
       "      <td>TK</td>\n",
       "      <td>294</td>\n",
       "      <td>51</td>\n",
       "      <td>5.764706</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>17. Art forms that appeal to modern leftist in...</td>\n",
       "      <td>TK</td>\n",
       "      <td>331</td>\n",
       "      <td>55</td>\n",
       "      <td>6.018182</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>27.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>201. Suppose for example that the revolutionar...</td>\n",
       "      <td>TK</td>\n",
       "      <td>837</td>\n",
       "      <td>133</td>\n",
       "      <td>6.293233</td>\n",
       "      <td>0.563910</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>14.777778</td>\n",
       "      <td>14.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>71. People have many transitory drives or impu...</td>\n",
       "      <td>TK</td>\n",
       "      <td>881</td>\n",
       "      <td>156</td>\n",
       "      <td>5.647436</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>19.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>153. Thus control over human behavior will be ...</td>\n",
       "      <td>TK</td>\n",
       "      <td>875</td>\n",
       "      <td>151</td>\n",
       "      <td>5.794702</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>30.200000</td>\n",
       "      <td>30.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>816 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text Author  \\\n",
       "0     Scoring in PROC DISCRIM is as easy as validati...     AM   \n",
       "1     In the GLM procedure, you may have used LSMEAN...     AM   \n",
       "2     The first problem, accuracy of the data file, ...     AM   \n",
       "3     If the homogeneity of covariance matrices assu...     AM   \n",
       "4     With a CONTRAST statement, you specify L, in t...     AM   \n",
       "...                                                 ...    ...   \n",
       "1917  6. Almost everyone will agree that we live in ...     TK   \n",
       "1918  17. Art forms that appeal to modern leftist in...     TK   \n",
       "1919  201. Suppose for example that the revolutionar...     TK   \n",
       "1920  71. People have many transitory drives or impu...     TK   \n",
       "1921  153. Thus control over human behavior will be ...     TK   \n",
       "\n",
       "      sentence_length  word_count  avg_word_length  type_to_token_by_author  \\\n",
       "0                 215          37         5.810811                 0.756757   \n",
       "1                 782         129         6.062016                 0.542636   \n",
       "2                 990         159         6.226415                 0.583851   \n",
       "3                 934         146         6.397260                 0.463087   \n",
       "4                1490         247         6.032389                 0.370518   \n",
       "...               ...         ...              ...                      ...   \n",
       "1917              294          51         5.764706                 0.680000   \n",
       "1918              331          55         6.018182                 0.851852   \n",
       "1919              837         133         6.293233                 0.563910   \n",
       "1920              881         156         5.647436                 0.602564   \n",
       "1921              875         151         5.794702                 0.594595   \n",
       "\n",
       "      Lexical_Diversity  comma_count  Avg_Sentence_Length  avg_sentence_length  \n",
       "0                   1.0            0            12.333333            12.333333  \n",
       "1                   1.0            4            18.428571            18.428571  \n",
       "2                   1.0            6            16.000000            16.000000  \n",
       "3                   1.0            4            18.250000            18.250000  \n",
       "4                   1.0            8            15.687500            15.687500  \n",
       "...                 ...          ...                  ...                  ...  \n",
       "1917                1.0            1            17.000000            17.000000  \n",
       "1918                1.0            3            27.500000            27.500000  \n",
       "1919                1.0            3            14.777778            14.777778  \n",
       "1920                1.0            7            19.500000            19.500000  \n",
       "1921                1.0            8            30.200000            30.200000  \n",
       "\n",
       "[816 rows x 10 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c27f19a4-1bed-4209-8f8f-ff22cb1a4894",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>type_to_token_by_author</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>comma_count</th>\n",
       "      <th>Avg_Sentence_Length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>POS_Nouns</th>\n",
       "      <th>POS_Verbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scoring in PROC DISCRIM is as easy as validati...</td>\n",
       "      <td>AM</td>\n",
       "      <td>215</td>\n",
       "      <td>37</td>\n",
       "      <td>5.810811</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the GLM procedure, you may have used LSMEAN...</td>\n",
       "      <td>AM</td>\n",
       "      <td>782</td>\n",
       "      <td>129</td>\n",
       "      <td>6.062016</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>0.286822</td>\n",
       "      <td>0.093023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first problem, accuracy of the data file, ...</td>\n",
       "      <td>AM</td>\n",
       "      <td>990</td>\n",
       "      <td>159</td>\n",
       "      <td>6.226415</td>\n",
       "      <td>0.583851</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.234177</td>\n",
       "      <td>0.158228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the homogeneity of covariance matrices assu...</td>\n",
       "      <td>AM</td>\n",
       "      <td>934</td>\n",
       "      <td>146</td>\n",
       "      <td>6.397260</td>\n",
       "      <td>0.463087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>0.312925</td>\n",
       "      <td>0.081633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With a CONTRAST statement, you specify L, in t...</td>\n",
       "      <td>AM</td>\n",
       "      <td>1490</td>\n",
       "      <td>247</td>\n",
       "      <td>6.032389</td>\n",
       "      <td>0.370518</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>15.687500</td>\n",
       "      <td>15.687500</td>\n",
       "      <td>0.282869</td>\n",
       "      <td>0.127490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author  sentence_length  \\\n",
       "0  Scoring in PROC DISCRIM is as easy as validati...     AM              215   \n",
       "1  In the GLM procedure, you may have used LSMEAN...     AM              782   \n",
       "2  The first problem, accuracy of the data file, ...     AM              990   \n",
       "3  If the homogeneity of covariance matrices assu...     AM              934   \n",
       "4  With a CONTRAST statement, you specify L, in t...     AM             1490   \n",
       "\n",
       "   word_count  avg_word_length  type_to_token_by_author  Lexical_Diversity  \\\n",
       "0          37         5.810811                 0.756757                1.0   \n",
       "1         129         6.062016                 0.542636                1.0   \n",
       "2         159         6.226415                 0.583851                1.0   \n",
       "3         146         6.397260                 0.463087                1.0   \n",
       "4         247         6.032389                 0.370518                1.0   \n",
       "\n",
       "   comma_count  Avg_Sentence_Length  avg_sentence_length  POS_Nouns  POS_Verbs  \n",
       "0            0            12.333333            12.333333   0.305556   0.166667  \n",
       "1            4            18.428571            18.428571   0.286822   0.093023  \n",
       "2            6            16.000000            16.000000   0.234177   0.158228  \n",
       "3            4            18.250000            18.250000   0.312925   0.081633  \n",
       "4            8            15.687500            15.687500   0.282869   0.127490  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def pos_proportions(text, pos_tag):\n",
    "    doc = nlp(text)\n",
    "    pos_counts = sum(1 for token in doc if token.pos_ == pos_tag)\n",
    "    total_words = sum(1 for token in doc if token.is_alpha)\n",
    "    return pos_counts / total_words if total_words > 0 else 0\n",
    "\n",
    "\n",
    "df_train['POS_Nouns'] = df_train['Text'].apply(lambda x: pos_proportions(x, \"NOUN\"))\n",
    "df_train['POS_Verbs'] = df_train['Text'].apply(lambda x: pos_proportions(x, \"VERB\"))\n",
    "\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b66965c7-2e93-41e9-9525-60f92dac4f13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>type_to_token_by_author</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>comma_count</th>\n",
       "      <th>Avg_Sentence_Length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>POS_Nouns</th>\n",
       "      <th>POS_Verbs</th>\n",
       "      <th>flesch_reading_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scoring in PROC DISCRIM is as easy as validati...</td>\n",
       "      <td>AM</td>\n",
       "      <td>215</td>\n",
       "      <td>37</td>\n",
       "      <td>5.810811</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>58.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the GLM procedure, you may have used LSMEAN...</td>\n",
       "      <td>AM</td>\n",
       "      <td>782</td>\n",
       "      <td>129</td>\n",
       "      <td>6.062016</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>0.286822</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>52.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first problem, accuracy of the data file, ...</td>\n",
       "      <td>AM</td>\n",
       "      <td>990</td>\n",
       "      <td>159</td>\n",
       "      <td>6.226415</td>\n",
       "      <td>0.583851</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.234177</td>\n",
       "      <td>0.158228</td>\n",
       "      <td>46.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the homogeneity of covariance matrices assu...</td>\n",
       "      <td>AM</td>\n",
       "      <td>934</td>\n",
       "      <td>146</td>\n",
       "      <td>6.397260</td>\n",
       "      <td>0.463087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>0.312925</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>44.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With a CONTRAST statement, you specify L, in t...</td>\n",
       "      <td>AM</td>\n",
       "      <td>1490</td>\n",
       "      <td>247</td>\n",
       "      <td>6.032389</td>\n",
       "      <td>0.370518</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>15.687500</td>\n",
       "      <td>15.687500</td>\n",
       "      <td>0.282869</td>\n",
       "      <td>0.127490</td>\n",
       "      <td>55.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author  sentence_length  \\\n",
       "0  Scoring in PROC DISCRIM is as easy as validati...     AM              215   \n",
       "1  In the GLM procedure, you may have used LSMEAN...     AM              782   \n",
       "2  The first problem, accuracy of the data file, ...     AM              990   \n",
       "3  If the homogeneity of covariance matrices assu...     AM              934   \n",
       "4  With a CONTRAST statement, you specify L, in t...     AM             1490   \n",
       "\n",
       "   word_count  avg_word_length  type_to_token_by_author  Lexical_Diversity  \\\n",
       "0          37         5.810811                 0.756757                1.0   \n",
       "1         129         6.062016                 0.542636                1.0   \n",
       "2         159         6.226415                 0.583851                1.0   \n",
       "3         146         6.397260                 0.463087                1.0   \n",
       "4         247         6.032389                 0.370518                1.0   \n",
       "\n",
       "   comma_count  Avg_Sentence_Length  avg_sentence_length  POS_Nouns  \\\n",
       "0            0            12.333333            12.333333   0.305556   \n",
       "1            4            18.428571            18.428571   0.286822   \n",
       "2            6            16.000000            16.000000   0.234177   \n",
       "3            4            18.250000            18.250000   0.312925   \n",
       "4            8            15.687500            15.687500   0.282869   \n",
       "\n",
       "   POS_Verbs  flesch_reading_score  \n",
       "0   0.166667                 58.99  \n",
       "1   0.093023                 52.80  \n",
       "2   0.158228                 46.88  \n",
       "3   0.081633                 44.44  \n",
       "4   0.127490                 55.54  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flesch_reading_score(text):\n",
    "    return textstat.flesch_reading_ease(text)\n",
    "\n",
    "\n",
    "df_train['flesch_reading_score'] = df_train['Text'].apply(flesch_reading_score)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba27dc71-88f7-499c-9fc3-62437efe9de8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>type_to_token_by_author</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>comma_count</th>\n",
       "      <th>Avg_Sentence_Length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>POS_Nouns</th>\n",
       "      <th>POS_Verbs</th>\n",
       "      <th>flesch_reading_score</th>\n",
       "      <th>Unique_Word_Count</th>\n",
       "      <th>Exclamation_Count</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scoring in PROC DISCRIM is as easy as validati...</td>\n",
       "      <td>AM</td>\n",
       "      <td>215</td>\n",
       "      <td>37</td>\n",
       "      <td>5.810811</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>58.99</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the GLM procedure, you may have used LSMEAN...</td>\n",
       "      <td>AM</td>\n",
       "      <td>782</td>\n",
       "      <td>129</td>\n",
       "      <td>6.062016</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>0.286822</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>52.80</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first problem, accuracy of the data file, ...</td>\n",
       "      <td>AM</td>\n",
       "      <td>990</td>\n",
       "      <td>159</td>\n",
       "      <td>6.226415</td>\n",
       "      <td>0.583851</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.234177</td>\n",
       "      <td>0.158228</td>\n",
       "      <td>46.88</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the homogeneity of covariance matrices assu...</td>\n",
       "      <td>AM</td>\n",
       "      <td>934</td>\n",
       "      <td>146</td>\n",
       "      <td>6.397260</td>\n",
       "      <td>0.463087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>0.312925</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>44.44</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With a CONTRAST statement, you specify L, in t...</td>\n",
       "      <td>AM</td>\n",
       "      <td>1490</td>\n",
       "      <td>247</td>\n",
       "      <td>6.032389</td>\n",
       "      <td>0.370518</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>15.687500</td>\n",
       "      <td>15.687500</td>\n",
       "      <td>0.282869</td>\n",
       "      <td>0.127490</td>\n",
       "      <td>55.54</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author  sentence_length  \\\n",
       "0  Scoring in PROC DISCRIM is as easy as validati...     AM              215   \n",
       "1  In the GLM procedure, you may have used LSMEAN...     AM              782   \n",
       "2  The first problem, accuracy of the data file, ...     AM              990   \n",
       "3  If the homogeneity of covariance matrices assu...     AM              934   \n",
       "4  With a CONTRAST statement, you specify L, in t...     AM             1490   \n",
       "\n",
       "   word_count  avg_word_length  type_to_token_by_author  Lexical_Diversity  \\\n",
       "0          37         5.810811                 0.756757                1.0   \n",
       "1         129         6.062016                 0.542636                1.0   \n",
       "2         159         6.226415                 0.583851                1.0   \n",
       "3         146         6.397260                 0.463087                1.0   \n",
       "4         247         6.032389                 0.370518                1.0   \n",
       "\n",
       "   comma_count  Avg_Sentence_Length  avg_sentence_length  POS_Nouns  \\\n",
       "0            0            12.333333            12.333333   0.305556   \n",
       "1            4            18.428571            18.428571   0.286822   \n",
       "2            6            16.000000            16.000000   0.234177   \n",
       "3            4            18.250000            18.250000   0.312925   \n",
       "4            8            15.687500            15.687500   0.282869   \n",
       "\n",
       "   POS_Verbs  flesch_reading_score  Unique_Word_Count  Exclamation_Count  \\\n",
       "0   0.166667                 58.99                 32                  0   \n",
       "1   0.093023                 52.80                 84                  0   \n",
       "2   0.158228                 46.88                110                  1   \n",
       "3   0.081633                 44.44                 82                  0   \n",
       "4   0.127490                 55.54                120                  0   \n",
       "\n",
       "   exclamation_count  unique_word_count  \n",
       "0                  0                 32  \n",
       "1                  0                 84  \n",
       "2                  1                110  \n",
       "3                  0                 82  \n",
       "4                  0                120  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unique_word_count(text):\n",
    "    words = text.split()\n",
    "    return len(set(words))\n",
    "\n",
    "df_train['unique_word_count'] = df_train['Text'].apply(unique_word_count)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24124b90-1a64-4608-8a96-605467fdcc7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>type_to_token_by_author</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>comma_count</th>\n",
       "      <th>Avg_Sentence_Length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>POS_Nouns</th>\n",
       "      <th>POS_Verbs</th>\n",
       "      <th>flesch_reading_score</th>\n",
       "      <th>Unique_Word_Count</th>\n",
       "      <th>Exclamation_Count</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scoring in PROC DISCRIM is as easy as validati...</td>\n",
       "      <td>AM</td>\n",
       "      <td>215</td>\n",
       "      <td>37</td>\n",
       "      <td>5.810811</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>58.99</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the GLM procedure, you may have used LSMEAN...</td>\n",
       "      <td>AM</td>\n",
       "      <td>782</td>\n",
       "      <td>129</td>\n",
       "      <td>6.062016</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>0.286822</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>52.80</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first problem, accuracy of the data file, ...</td>\n",
       "      <td>AM</td>\n",
       "      <td>990</td>\n",
       "      <td>159</td>\n",
       "      <td>6.226415</td>\n",
       "      <td>0.583851</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.234177</td>\n",
       "      <td>0.158228</td>\n",
       "      <td>46.88</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the homogeneity of covariance matrices assu...</td>\n",
       "      <td>AM</td>\n",
       "      <td>934</td>\n",
       "      <td>146</td>\n",
       "      <td>6.397260</td>\n",
       "      <td>0.463087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>0.312925</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>44.44</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With a CONTRAST statement, you specify L, in t...</td>\n",
       "      <td>AM</td>\n",
       "      <td>1490</td>\n",
       "      <td>247</td>\n",
       "      <td>6.032389</td>\n",
       "      <td>0.370518</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>15.687500</td>\n",
       "      <td>15.687500</td>\n",
       "      <td>0.282869</td>\n",
       "      <td>0.127490</td>\n",
       "      <td>55.54</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author  sentence_length  \\\n",
       "0  Scoring in PROC DISCRIM is as easy as validati...     AM              215   \n",
       "1  In the GLM procedure, you may have used LSMEAN...     AM              782   \n",
       "2  The first problem, accuracy of the data file, ...     AM              990   \n",
       "3  If the homogeneity of covariance matrices assu...     AM              934   \n",
       "4  With a CONTRAST statement, you specify L, in t...     AM             1490   \n",
       "\n",
       "   word_count  avg_word_length  type_to_token_by_author  Lexical_Diversity  \\\n",
       "0          37         5.810811                 0.756757                1.0   \n",
       "1         129         6.062016                 0.542636                1.0   \n",
       "2         159         6.226415                 0.583851                1.0   \n",
       "3         146         6.397260                 0.463087                1.0   \n",
       "4         247         6.032389                 0.370518                1.0   \n",
       "\n",
       "   comma_count  Avg_Sentence_Length  avg_sentence_length  POS_Nouns  \\\n",
       "0            0            12.333333            12.333333   0.305556   \n",
       "1            4            18.428571            18.428571   0.286822   \n",
       "2            6            16.000000            16.000000   0.234177   \n",
       "3            4            18.250000            18.250000   0.312925   \n",
       "4            8            15.687500            15.687500   0.282869   \n",
       "\n",
       "   POS_Verbs  flesch_reading_score  Unique_Word_Count  Exclamation_Count  \\\n",
       "0   0.166667                 58.99                 32                  0   \n",
       "1   0.093023                 52.80                 84                  0   \n",
       "2   0.158228                 46.88                110                  1   \n",
       "3   0.081633                 44.44                 82                  0   \n",
       "4   0.127490                 55.54                120                  0   \n",
       "\n",
       "   exclamation_count  unique_word_count  \n",
       "0                  0                 32  \n",
       "1                  0                 84  \n",
       "2                  1                110  \n",
       "3                  0                 82  \n",
       "4                  0                120  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_exclamations(text):\n",
    "    return text.count('!')\n",
    "\n",
    "df_train['exclamation_count'] = df_train['Text'].apply(count_exclamations)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8577c0c8-e032-4180-8009-4dc9950a27a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>type_to_token_by_author</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>comma_count</th>\n",
       "      <th>Avg_Sentence_Length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>POS_Nouns</th>\n",
       "      <th>POS_Verbs</th>\n",
       "      <th>flesch_reading_score</th>\n",
       "      <th>Unique_Word_Count</th>\n",
       "      <th>Exclamation_Count</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>Gunning_Fog_Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scoring in PROC DISCRIM is as easy as validati...</td>\n",
       "      <td>AM</td>\n",
       "      <td>215</td>\n",
       "      <td>37</td>\n",
       "      <td>5.810811</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>58.99</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>10.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the GLM procedure, you may have used LSMEAN...</td>\n",
       "      <td>AM</td>\n",
       "      <td>782</td>\n",
       "      <td>129</td>\n",
       "      <td>6.062016</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>0.286822</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>52.80</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>11.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first problem, accuracy of the data file, ...</td>\n",
       "      <td>AM</td>\n",
       "      <td>990</td>\n",
       "      <td>159</td>\n",
       "      <td>6.226415</td>\n",
       "      <td>0.583851</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.234177</td>\n",
       "      <td>0.158228</td>\n",
       "      <td>46.88</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>11.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the homogeneity of covariance matrices assu...</td>\n",
       "      <td>AM</td>\n",
       "      <td>934</td>\n",
       "      <td>146</td>\n",
       "      <td>6.397260</td>\n",
       "      <td>0.463087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>0.312925</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>44.44</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With a CONTRAST statement, you specify L, in t...</td>\n",
       "      <td>AM</td>\n",
       "      <td>1490</td>\n",
       "      <td>247</td>\n",
       "      <td>6.032389</td>\n",
       "      <td>0.370518</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>15.687500</td>\n",
       "      <td>15.687500</td>\n",
       "      <td>0.282869</td>\n",
       "      <td>0.127490</td>\n",
       "      <td>55.54</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>9.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author  sentence_length  \\\n",
       "0  Scoring in PROC DISCRIM is as easy as validati...     AM              215   \n",
       "1  In the GLM procedure, you may have used LSMEAN...     AM              782   \n",
       "2  The first problem, accuracy of the data file, ...     AM              990   \n",
       "3  If the homogeneity of covariance matrices assu...     AM              934   \n",
       "4  With a CONTRAST statement, you specify L, in t...     AM             1490   \n",
       "\n",
       "   word_count  avg_word_length  type_to_token_by_author  Lexical_Diversity  \\\n",
       "0          37         5.810811                 0.756757                1.0   \n",
       "1         129         6.062016                 0.542636                1.0   \n",
       "2         159         6.226415                 0.583851                1.0   \n",
       "3         146         6.397260                 0.463087                1.0   \n",
       "4         247         6.032389                 0.370518                1.0   \n",
       "\n",
       "   comma_count  Avg_Sentence_Length  avg_sentence_length  POS_Nouns  \\\n",
       "0            0            12.333333            12.333333   0.305556   \n",
       "1            4            18.428571            18.428571   0.286822   \n",
       "2            6            16.000000            16.000000   0.234177   \n",
       "3            4            18.250000            18.250000   0.312925   \n",
       "4            8            15.687500            15.687500   0.282869   \n",
       "\n",
       "   POS_Verbs  flesch_reading_score  Unique_Word_Count  Exclamation_Count  \\\n",
       "0   0.166667                 58.99                 32                  0   \n",
       "1   0.093023                 52.80                 84                  0   \n",
       "2   0.158228                 46.88                110                  1   \n",
       "3   0.081633                 44.44                 82                  0   \n",
       "4   0.127490                 55.54                120                  0   \n",
       "\n",
       "   exclamation_count  unique_word_count  Gunning_Fog_Index  \n",
       "0                  0                 32              10.33  \n",
       "1                  0                 84              11.39  \n",
       "2                  1                110              11.39  \n",
       "3                  0                 82              11.98  \n",
       "4                  0                120               9.15  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value to check how complex the text is\n",
    "df_train['gunning_fog_index'] = df_train['Text'].apply(textstat.gunning_fog)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993bcfdf-0683-4a36-a66b-63081eca4fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "sns.countplot(data=df_train, x=\"Author\", order=df_train[\"Author\"].value_counts().index, palette=palette);\n",
    "plt.title(\"Target Variable Distribution\");\n",
    "plt.ylabel(\"Author Counts\");\n",
    "plt.xlabel(\"Authors\");\n",
    "plt.xticks();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc3760-a5f1-4bf3-824c-d116df074ae2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    Data is imbalanced, need to do a combination of oversampling and undersampling to have equal representation of each class (author) and ensure that our model shows no bias. The reason for using combination of oversampling and undersampling is because making use of just undersampling techniques will reduce the amount of data provided and doing just oversampling will run the risk of overfitting the model, as it might start to \"memorize\" the oversampled data instead of learning to generalize from patterns in the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a5f27-85e0-433c-8b0f-0fc97361a14f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sns.boxplot(data=df_train, x=df_train.Author, y=df_train.length)\n",
    "sns.violinplot(data=df_train, x=df_train.Author, y=df_train.length, inner=\"points\", palette=palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e6ace-f73a-46a4-adc1-f80221ef851d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    Presence of outliers indicates that texts which are unusually long or short could indicate the author's sentiment more strongly. The violin plot shows that majority of the text lengths are around 400 - 500 characters.\n",
    "</div>\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    Given the differences in the text lengths, it would be best to normalize the text to control the author specific length effects.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d04e30e-a2c1-4421-b003-92bf6ab1a6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \" \".join(df_train.Text)\n",
    "\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3cfcba-5898-4790-8de6-94a2ac6b0b21",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "At first glance it can be seen that: \n",
    "    <br>\n",
    "    -> Most important words: \"Model\", \"Data\", \"Analysis\", \"Variable\" <br>\n",
    "    -> Stopwords: \"One\", \"May\", \"Even\", \"Must\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1277a-f957-4551-b3cb-f0eb74e1be5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# look at the words that each author has written\n",
    "\n",
    "author_texts = {author: \" \".join(df_train[df_train['Author'] == author]['Text']) for author in df_train.Author.unique()}\n",
    "\n",
    "fig = plt.figure(figsize = (8,4))\n",
    "\n",
    "\n",
    "for author, text in author_texts.items():\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.subplot(122)\n",
    "    plt.imshow(wordcloud) \n",
    "    plt.title(f\"Text by {author}\")\n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31cc6d6-3dac-4ab2-ab41-965dcb7406ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "At first glance it can be seen that each author is speaking about different topics: \n",
    "    <br>\n",
    "    -> AM: talks about statistical classification techniques, discussing things such as discriminant analysis, group differences, and variable selection <br>\n",
    "    -> CD: talks about machine learning and probability concepts like logistic regression, odds ratios, model estimation, predictor variable <br>\n",
    "    -> DM: talks about data modeling, specifically addressing the management and analysis of transactional data sets <br>\n",
    "    -> DO: talks about experimental design and topics like block design and factorial experiements <br>\n",
    "    -> FE: talks about time series forecasting with topics like PROC, ARIMA and forecast <br>\n",
    "    -> TK: talks about societal impact, with topics like society, psychological, and people.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c266161-fdec-43ab-ba84-c33c7d1188b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718da763-e5aa-4e3b-9282-9f0f96b8b887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(txt):\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    txt_clean = txt.replace('\\t', ' ').replace('\\n', ' ')\n",
    "    txt_tokens = txt_clean.split(\" \")\n",
    "    txt_lowercase = [word.lower() for word in txt_tokens]\n",
    "    txt_no_punctuation = [word.translate(str.maketrans('', '', string.punctuation)) for word in txt_lowercase]\n",
    "    txt_lemmatize = [lemmatiser.lemmatize(word) for word in txt_no_punctuation]\n",
    "    txt_no_stopwords = [word for word in txt_lemmatize if word not in stopwords.words('english') and word != \"\"]\n",
    "    return txt_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d184dda7-4b27-4c91-bdb4-38c855a60950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train[\"preprocessed_text\"] = df_train[\"Text\"].apply(lambda x: text_preprocessing(x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc09eb-a3d3-4288-b07a-b9acff110ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped = df_train.groupby('Author')['preprocessed_text'].sum()\n",
    "\n",
    "for author, words in grouped.items():\n",
    "    text = ' '.join(words)\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "    plt.figure(figsize = (8, 8)) \n",
    "    plt.subplot(122)\n",
    "    plt.imshow(wordcloud) \n",
    "    plt.title(f\"Text by {author}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0270c22-2500-4d75-a1dd-0a7f5c46e70a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tokens = sum(df_train['preprocessed_text'], [])\n",
    "unigram_freq = FreqDist(all_tokens)\n",
    "\n",
    "unigrams, counts = zip(*unigram_freq.most_common(15))\n",
    "plt.barh(unigrams, counts)\n",
    "plt.title('TOP 15 UNIGRAMS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d30cf9-ab31-44a2-8df7-d99419d6ebdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_bigrams = list(bigrams(all_tokens))\n",
    "bigram_freq = FreqDist(all_bigrams)\n",
    "print(bigram_freq.most_common(10))\n",
    "\n",
    "bigrams, counts = zip(*bigram_freq.most_common(15))\n",
    "bigram_labels = [' '.join(bigram) for bigram in bigrams]\n",
    "plt.barh(bigram_labels, counts)\n",
    "plt.title('Top 15 BIGRAMS')\n",
    "plt.figure(figsize = (15, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa25e91-8eb9-445a-994f-766c3250dd2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_trigrams = list(trigrams(all_tokens))\n",
    "all_trigrams\n",
    "trigram_freq = FreqDist(all_trigrams)\n",
    "print(trigram_freq.most_common(15))\n",
    "\n",
    "trigrams, counts = zip(*trigram_freq.most_common(15))\n",
    "trigram_labels = [' '.join(trigram) for trigram in trigrams]\n",
    "plt.barh(trigram_labels, counts)\n",
    "plt.title('TOP 15 TRIGRAMS')\n",
    "plt.figure(figsize = (15, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546317a-58b9-4151-801a-2d68a87c81e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compound_words = [i[0] for i in bigram_freq.most_common(15)]\n",
    "compound_words += [('logistic', 'regression', 'model')]\n",
    "mwe_tokenizer = MWETokenizer(compound_words, separator=\"_\")\n",
    "mwe_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb2e14f-2d19-407d-ae2a-dcdf00225c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use this to update the tokens with compound words found from the bigram analysis\n",
    "# also remove special characters and numbers\n",
    "def apply_mwe_to_tokens(row):\n",
    "    tokens = mwe_tokenizer.tokenize(row)\n",
    "    cleaned_tokens = [token.replace('\\t', '') for token in tokens if not token.isnumeric()]\n",
    "    return cleaned_tokens\n",
    "\n",
    "df_train['preprocessed_text'] = df_train['preprocessed_text'].apply(apply_mwe_to_tokens)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfd724-7695-4558-8d54-b83a9ceeedd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.to_csv(\"cleaned_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
