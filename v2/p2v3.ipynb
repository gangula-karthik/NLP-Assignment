{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textstat\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import ast\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "sns.set_style(\"darkgrid\")\n",
    "from pprint import pprint\n",
    "palette = \"cool\"\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore')\n",
    "# warnings.resetwarnings() to reset\n",
    "nlp = spacy.load(\"en_core_web_sm\") # spacy english model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>type_to_token_ratio</th>\n",
       "      <th>punctuations_count</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>POS_Nouns</th>\n",
       "      <th>POS_Verbs</th>\n",
       "      <th>flesch_reading_score</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>stopwords_number</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scoring in PROC DISCRIM is as easy as validati...</td>\n",
       "      <td>AM</td>\n",
       "      <td>215</td>\n",
       "      <td>37</td>\n",
       "      <td>5.810811</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>5</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>58.99</td>\n",
       "      <td>10.33</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>['scoring', 'proc', 'discrim', 'easy', 'valida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the GLM procedure, you may have used LSMEAN...</td>\n",
       "      <td>AM</td>\n",
       "      <td>782</td>\n",
       "      <td>129</td>\n",
       "      <td>6.062016</td>\n",
       "      <td>0.661972</td>\n",
       "      <td>11</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>0.286822</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>52.80</td>\n",
       "      <td>11.39</td>\n",
       "      <td>47</td>\n",
       "      <td>59</td>\n",
       "      <td>['glm', 'procedure', 'may', 'used', 'lsmeans',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first problem, accuracy of the data file, ...</td>\n",
       "      <td>AM</td>\n",
       "      <td>990</td>\n",
       "      <td>159</td>\n",
       "      <td>6.226415</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>19</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.234177</td>\n",
       "      <td>0.158228</td>\n",
       "      <td>46.88</td>\n",
       "      <td>11.39</td>\n",
       "      <td>67</td>\n",
       "      <td>66</td>\n",
       "      <td>['first', 'problem', 'accuracy', 'data', 'file...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the homogeneity of covariance matrices assu...</td>\n",
       "      <td>AM</td>\n",
       "      <td>934</td>\n",
       "      <td>146</td>\n",
       "      <td>6.397260</td>\n",
       "      <td>0.604938</td>\n",
       "      <td>17</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>0.312925</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>44.44</td>\n",
       "      <td>11.98</td>\n",
       "      <td>49</td>\n",
       "      <td>69</td>\n",
       "      <td>['homogeneity', 'covariance', 'matrix', 'assum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With a CONTRAST statement, you specify L, in t...</td>\n",
       "      <td>AM</td>\n",
       "      <td>1490</td>\n",
       "      <td>247</td>\n",
       "      <td>6.032389</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>29</td>\n",
       "      <td>15.687500</td>\n",
       "      <td>0.282869</td>\n",
       "      <td>0.127490</td>\n",
       "      <td>55.54</td>\n",
       "      <td>9.15</td>\n",
       "      <td>63</td>\n",
       "      <td>122</td>\n",
       "      <td>['contrast', 'statement', 'specify', 'l', 'cas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author  text_length  \\\n",
       "0  Scoring in PROC DISCRIM is as easy as validati...     AM          215   \n",
       "1  In the GLM procedure, you may have used LSMEAN...     AM          782   \n",
       "2  The first problem, accuracy of the data file, ...     AM          990   \n",
       "3  If the homogeneity of covariance matrices assu...     AM          934   \n",
       "4  With a CONTRAST statement, you specify L, in t...     AM         1490   \n",
       "\n",
       "   word_count  avg_word_length  type_to_token_ratio  punctuations_count  \\\n",
       "0          37         5.810811             0.840000                   5   \n",
       "1         129         6.062016             0.661972                  11   \n",
       "2         159         6.226415             0.712766                  19   \n",
       "3         146         6.397260             0.604938                  17   \n",
       "4         247         6.032389             0.488372                  29   \n",
       "\n",
       "   avg_sentence_length  POS_Nouns  POS_Verbs  flesch_reading_score  \\\n",
       "0            12.333333   0.305556   0.166667                 58.99   \n",
       "1            18.428571   0.286822   0.093023                 52.80   \n",
       "2            16.000000   0.234177   0.158228                 46.88   \n",
       "3            18.250000   0.312925   0.081633                 44.44   \n",
       "4            15.687500   0.282869   0.127490                 55.54   \n",
       "\n",
       "   gunning_fog_index  unique_word_count  stopwords_number  \\\n",
       "0              10.33                 21                12   \n",
       "1              11.39                 47                59   \n",
       "2              11.39                 67                66   \n",
       "3              11.98                 49                69   \n",
       "4               9.15                 63               122   \n",
       "\n",
       "                                      cleaned_tokens  \n",
       "0  ['scoring', 'proc', 'discrim', 'easy', 'valida...  \n",
       "1  ['glm', 'procedure', 'may', 'used', 'lsmeans',...  \n",
       "2  ['first', 'problem', 'accuracy', 'data', 'file...  \n",
       "3  ['homogeneity', 'covariance', 'matrix', 'assum...  \n",
       "4  ['contrast', 'statement', 'specify', 'l', 'cas...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"cleaned_data.csv\", index_col=0)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"cleaned_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(df_train['cleaned_tokens'])\n",
    "y = df_train['Author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('              precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '          AM       1.00      0.38      0.56        13\\n'\n",
      " '          CD       0.85      1.00      0.92        35\\n'\n",
      " '          DM       0.88      0.97      0.92        29\\n'\n",
      " '          DO       1.00      0.31      0.47        13\\n'\n",
      " '          FE       0.93      0.93      0.93        28\\n'\n",
      " '          TK       0.85      1.00      0.92        46\\n'\n",
      " '\\n'\n",
      " '    accuracy                           0.88       164\\n'\n",
      " '   macro avg       0.92      0.76      0.79       164\\n'\n",
      " 'weighted avg       0.89      0.88      0.86       164\\n')\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = model.predict(X_val)\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "pprint(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>type_to_token_ratio</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>POS_Nouns</th>\n",
       "      <th>POS_Verbs</th>\n",
       "      <th>flesch_reading_score</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>stopwords_number</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scoring in PROC DISCRIM is as easy as validati...</td>\n",
       "      <td>AM</td>\n",
       "      <td>5.810811</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>58.99</td>\n",
       "      <td>10.33</td>\n",
       "      <td>12</td>\n",
       "      <td>['scoring', 'proc', 'discrim', 'easy', 'valida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the GLM procedure, you may have used LSMEAN...</td>\n",
       "      <td>AM</td>\n",
       "      <td>6.062016</td>\n",
       "      <td>0.661972</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>0.286822</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>52.80</td>\n",
       "      <td>11.39</td>\n",
       "      <td>59</td>\n",
       "      <td>['glm', 'procedure', 'may', 'used', 'lsmeans',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first problem, accuracy of the data file, ...</td>\n",
       "      <td>AM</td>\n",
       "      <td>6.226415</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.234177</td>\n",
       "      <td>0.158228</td>\n",
       "      <td>46.88</td>\n",
       "      <td>11.39</td>\n",
       "      <td>66</td>\n",
       "      <td>['first', 'problem', 'accuracy', 'data', 'file...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the homogeneity of covariance matrices assu...</td>\n",
       "      <td>AM</td>\n",
       "      <td>6.397260</td>\n",
       "      <td>0.604938</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>0.312925</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>44.44</td>\n",
       "      <td>11.98</td>\n",
       "      <td>69</td>\n",
       "      <td>['homogeneity', 'covariance', 'matrix', 'assum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With a CONTRAST statement, you specify L, in t...</td>\n",
       "      <td>AM</td>\n",
       "      <td>6.032389</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>15.687500</td>\n",
       "      <td>0.282869</td>\n",
       "      <td>0.127490</td>\n",
       "      <td>55.54</td>\n",
       "      <td>9.15</td>\n",
       "      <td>122</td>\n",
       "      <td>['contrast', 'statement', 'specify', 'l', 'cas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author  avg_word_length  \\\n",
       "0  Scoring in PROC DISCRIM is as easy as validati...     AM         5.810811   \n",
       "1  In the GLM procedure, you may have used LSMEAN...     AM         6.062016   \n",
       "2  The first problem, accuracy of the data file, ...     AM         6.226415   \n",
       "3  If the homogeneity of covariance matrices assu...     AM         6.397260   \n",
       "4  With a CONTRAST statement, you specify L, in t...     AM         6.032389   \n",
       "\n",
       "   type_to_token_ratio  avg_sentence_length  POS_Nouns  POS_Verbs  \\\n",
       "0             0.840000            12.333333   0.305556   0.166667   \n",
       "1             0.661972            18.428571   0.286822   0.093023   \n",
       "2             0.712766            16.000000   0.234177   0.158228   \n",
       "3             0.604938            18.250000   0.312925   0.081633   \n",
       "4             0.488372            15.687500   0.282869   0.127490   \n",
       "\n",
       "   flesch_reading_score  gunning_fog_index  stopwords_number  \\\n",
       "0                 58.99              10.33                12   \n",
       "1                 52.80              11.39                59   \n",
       "2                 46.88              11.39                66   \n",
       "3                 44.44              11.98                69   \n",
       "4                 55.54               9.15               122   \n",
       "\n",
       "                                      cleaned_tokens  \n",
       "0  ['scoring', 'proc', 'discrim', 'easy', 'valida...  \n",
       "1  ['glm', 'procedure', 'may', 'used', 'lsmeans',...  \n",
       "2  ['first', 'problem', 'accuracy', 'data', 'file...  \n",
       "3  ['homogeneity', 'covariance', 'matrix', 'assum...  \n",
       "4  ['contrast', 'statement', 'specify', 'l', 'cas...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_columns = ['text_length', 'unique_word_count', 'word_count', 'punctuations_count']\n",
    "tfidf_features_addon = df_train.drop(columns=feat_columns)\n",
    "tfidf_features_addon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<816x5926 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 40980 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf = tfidf_vectorizer.fit_transform(df_train['cleaned_tokens'])\n",
    "numerical_features = tfidf_features_addon.drop(columns=[\"Author\", \"cleaned_tokens\", \"Text\"])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "numerical_features = numerical_features.apply(pd.to_numeric, errors='coerce')\n",
    "numerical_features = numerical_features.fillna(0)\n",
    "numerical_features_scaled = scaler.fit_transform(numerical_features)\n",
    "\n",
    "\n",
    "X_combined = hstack([X_tfidf, numerical_features_scaled])\n",
    "X_combined\n",
    "\n",
    "# features are now combined, time to train the logisitc regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.910071621925959\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = model.predict(X_val)\n",
    "report = f1_score(y_val, y_val_pred, average=\"weighted\")\n",
    "pprint(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For time-dependent predictor variables (variab...</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The next task after settling upon a data acces...</td>\n",
       "      <td>DM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There are many techniques used to recode non-n...</td>\n",
       "      <td>DM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PROC AUTOREG can be used to illustrate the cal...</td>\n",
       "      <td>FE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A stratified sample is drawn from the non-INS ...</td>\n",
       "      <td>DM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author\n",
       "0  For time-dependent predictor variables (variab...     CD\n",
       "1  The next task after settling upon a data acces...     DM\n",
       "2  There are many techniques used to recode non-n...     DM\n",
       "3  PROC AUTOREG can be used to illustrate the cal...     FE\n",
       "4  A stratified sample is drawn from the non-INS ...     DM"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_excel(\"Assignment_Data/Data_test.xlsx\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>type_to_token_ratio</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>POS_Nouns</th>\n",
       "      <th>POS_Verbs</th>\n",
       "      <th>flesch_reading_score</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>stopwords_number</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For time-dependent predictor variables (variab...</td>\n",
       "      <td>CD</td>\n",
       "      <td>6.440860</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.139785</td>\n",
       "      <td>31.55</td>\n",
       "      <td>17.99</td>\n",
       "      <td>37</td>\n",
       "      <td>[time, dependent, predictor, variable, variabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The next task after settling upon a data acces...</td>\n",
       "      <td>DM</td>\n",
       "      <td>6.809524</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>113.600000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>33.24</td>\n",
       "      <td>16.50</td>\n",
       "      <td>29</td>\n",
       "      <td>[next, task, settling, upon, data, access, met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There are many techniques used to recode non-n...</td>\n",
       "      <td>DM</td>\n",
       "      <td>7.256757</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>88.666667</td>\n",
       "      <td>0.328767</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>28.20</td>\n",
       "      <td>13.45</td>\n",
       "      <td>24</td>\n",
       "      <td>[many, technique, used, recode, non, numeric, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PROC AUTOREG can be used to illustrate the cal...</td>\n",
       "      <td>FE</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>0.213115</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>42.41</td>\n",
       "      <td>14.68</td>\n",
       "      <td>25</td>\n",
       "      <td>[proc, autoreg, used, illustrate, calculation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A stratified sample is drawn from the non-INS ...</td>\n",
       "      <td>DM</td>\n",
       "      <td>7.379310</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>70.666667</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>44.71</td>\n",
       "      <td>10.78</td>\n",
       "      <td>9</td>\n",
       "      <td>[stratified, sample, drawn, non, in, client, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author  avg_word_length  \\\n",
       "0  For time-dependent predictor variables (variab...     CD         6.440860   \n",
       "1  The next task after settling upon a data acces...     DM         6.809524   \n",
       "2  There are many techniques used to recode non-n...     DM         7.256757   \n",
       "3  PROC AUTOREG can be used to illustrate the cal...     FE         6.000000   \n",
       "4  A stratified sample is drawn from the non-INS ...     DM         7.379310   \n",
       "\n",
       "   type_to_token_ratio  avg_sentence_length  POS_Nouns  POS_Verbs  \\\n",
       "0             0.818182           149.000000   0.268817   0.139785   \n",
       "1             0.836364           113.600000   0.357143   0.107143   \n",
       "2             0.769231            88.666667   0.328767   0.136986   \n",
       "3             0.861111            90.750000   0.213115   0.147541   \n",
       "4             0.909091            70.666667   0.241379   0.172414   \n",
       "\n",
       "   flesch_reading_score  gunning_fog_index  stopwords_number  \\\n",
       "0                 31.55              17.99                37   \n",
       "1                 33.24              16.50                29   \n",
       "2                 28.20              13.45                24   \n",
       "3                 42.41              14.68                25   \n",
       "4                 44.71              10.78                 9   \n",
       "\n",
       "                                      cleaned_tokens  \n",
       "0  [time, dependent, predictor, variable, variabl...  \n",
       "1  [next, task, settling, upon, data, access, met...  \n",
       "2  [many, technique, used, recode, non, numeric, ...  \n",
       "3  [proc, autoreg, used, illustrate, calculation,...  \n",
       "4  [stratified, sample, drawn, non, in, client, p...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # spacy english model\n",
    "\n",
    "def pos_proportions(text, pos_tag):\n",
    "    doc = nlp(text)\n",
    "    pos_counts = 0\n",
    "    total_words = 0\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            total_words += 1\n",
    "        if token.pos_ == pos_tag:\n",
    "            pos_counts += 1\n",
    "    return pos_counts / total_words if total_words > 0 else 0\n",
    "\n",
    "def avg_word_length(sent): \n",
    "    sent_len = len(sent.split(\" \"))\n",
    "    return sum([len(wrd)  for wrd in sent]) / sent_len\n",
    "\n",
    "def text_tokenizer(sent):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sent = sent.lower() # convert the text to lowercase\n",
    "    tokens = re.split(r'\\W+', sent) # split text based on non word characters\n",
    "    clean_tokens = [lemmatizer.lemmatize(i) for i in tokens if i not in string.punctuation or not i.isdigit()] # removing number, punctuation and lemmatizing the text\n",
    "    clean_tokens_no_stopwords = [i for i in clean_tokens if i not in stopwords.words('english')]\n",
    "    return clean_tokens_no_stopwords\n",
    "\n",
    "def calculate_ttr(text):\n",
    "    tokens = text_tokenizer(text.lower())\n",
    "    types = set(tokens)\n",
    "    return len(types) / len(tokens) if tokens else 0\n",
    "\n",
    "def creating_features():\n",
    "    df_test['avg_word_length'] = df_test['Text'].apply(avg_word_length)\n",
    "    df_test['type_to_token_ratio'] = df_test['Text'].apply(calculate_ttr)\n",
    "    df_test['avg_sentence_length'] = df_test['Text'].apply(lambda x: np.mean([len(s) for s in str(x).split('.')]))\n",
    "    df_test['POS_Nouns'] = df_test['Text'].apply(lambda x: pos_proportions(x, \"NOUN\"))\n",
    "    df_test['POS_Verbs'] = df_test['Text'].apply(lambda x: pos_proportions(x, \"VERB\"))\n",
    "    df_test['flesch_reading_score'] = df_test['Text'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "    df_test['gunning_fog_index'] = df_test['Text'].apply(lambda x: textstat.gunning_fog(x))\n",
    "    df_test['stopwords_number'] = df_test['Text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words(\"english\")]))\n",
    "    return df_test\n",
    "\n",
    "import contractions\n",
    "from collections import Counter\n",
    "\n",
    "remove_file_paths = lambda text: re.sub(r'\\b[A-Za-z]:\\\\[^ ]*|\\S+\\.\\S+', '', text).strip()\n",
    "\n",
    "def text_tokenizer(sent):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sent = remove_file_paths(sent) # removing all file names and file paths\n",
    "    sent = sent.lower() # text to lowercase\n",
    "    sent_expand = contractions.fix(sent) # expand the contractions (I'd => I would)\n",
    "    tokens = re.split(r'\\W+', sent_expand) # split based on non word characters\n",
    "    no_stopwords = [i for i in tokens if i not in stopwords.words('english')]\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(i) if i.lower() != \"sas\" else \"SAS\"\n",
    "        for i in no_stopwords \n",
    "        if i not in string.punctuation \n",
    "        and not any(char.isdigit() for char in i)  # Check for digits\n",
    "        and i != \"\"\n",
    "    ]# remove number, punctuation, remove words with numbers and lemmatize text\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def find_words_hyphen(text):\n",
    "    \"\"\"\n",
    "    finding all the words with hyphen and to make them as compound words when tokenizing\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'\\b[A-Za-z]+(?:-[A-Za-z]+)+\\b')\n",
    "    return pattern.findall(text)\n",
    "\n",
    "hyphenated_words = df_train['Text'].apply(find_words_hyphen).sum()\n",
    "hyphenated_words_flat = [''.join(word) for word in hyphenated_words]\n",
    "hyphenated_word_counts = Counter(hyphenated_words_flat)\n",
    "hyphenated_word_list = list(hyphenated_word_counts)\n",
    "\n",
    "\n",
    "df_test = creating_features()\n",
    "df_test[\"cleaned_tokens\"] = df_test[\"Text\"].apply(text_tokenizer)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>type_to_token_ratio</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>POS_Nouns</th>\n",
       "      <th>POS_Verbs</th>\n",
       "      <th>flesch_reading_score</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>stopwords_number</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For time-dependent predictor variables (variab...</td>\n",
       "      <td>CD</td>\n",
       "      <td>6.440860</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.139785</td>\n",
       "      <td>31.55</td>\n",
       "      <td>17.99</td>\n",
       "      <td>37</td>\n",
       "      <td>[time_dependent, predictor_variable, variable,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The next task after settling upon a data acces...</td>\n",
       "      <td>DM</td>\n",
       "      <td>6.809524</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>113.600000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>33.24</td>\n",
       "      <td>16.50</td>\n",
       "      <td>29</td>\n",
       "      <td>[next, task, settling, upon, data, access, met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There are many techniques used to recode non-n...</td>\n",
       "      <td>DM</td>\n",
       "      <td>7.256757</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>88.666667</td>\n",
       "      <td>0.328767</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>28.20</td>\n",
       "      <td>13.45</td>\n",
       "      <td>24</td>\n",
       "      <td>[many, technique, used, recode, non_numeric, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PROC AUTOREG can be used to illustrate the cal...</td>\n",
       "      <td>FE</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>0.213115</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>42.41</td>\n",
       "      <td>14.68</td>\n",
       "      <td>25</td>\n",
       "      <td>[proc, autoreg, used, illustrate, calculation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A stratified sample is drawn from the non-INS ...</td>\n",
       "      <td>DM</td>\n",
       "      <td>7.379310</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>70.666667</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>44.71</td>\n",
       "      <td>10.78</td>\n",
       "      <td>9</td>\n",
       "      <td>[stratified, sample, drawn, non, in, client, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author  avg_word_length  \\\n",
       "0  For time-dependent predictor variables (variab...     CD         6.440860   \n",
       "1  The next task after settling upon a data acces...     DM         6.809524   \n",
       "2  There are many techniques used to recode non-n...     DM         7.256757   \n",
       "3  PROC AUTOREG can be used to illustrate the cal...     FE         6.000000   \n",
       "4  A stratified sample is drawn from the non-INS ...     DM         7.379310   \n",
       "\n",
       "   type_to_token_ratio  avg_sentence_length  POS_Nouns  POS_Verbs  \\\n",
       "0             0.818182           149.000000   0.268817   0.139785   \n",
       "1             0.836364           113.600000   0.357143   0.107143   \n",
       "2             0.769231            88.666667   0.328767   0.136986   \n",
       "3             0.861111            90.750000   0.213115   0.147541   \n",
       "4             0.909091            70.666667   0.241379   0.172414   \n",
       "\n",
       "   flesch_reading_score  gunning_fog_index  stopwords_number  \\\n",
       "0                 31.55              17.99                37   \n",
       "1                 33.24              16.50                29   \n",
       "2                 28.20              13.45                24   \n",
       "3                 42.41              14.68                25   \n",
       "4                 44.71              10.78                 9   \n",
       "\n",
       "                                      cleaned_tokens  \n",
       "0  [time_dependent, predictor_variable, variable,...  \n",
       "1  [next, task, settling, upon, data, access, met...  \n",
       "2  [many, technique, used, recode, non_numeric, i...  \n",
       "3  [proc, autoreg, used, illustrate, calculation,...  \n",
       "4  [stratified, sample, drawn, non, in, client, p...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('mwe_tokenizer.pkl', 'rb') as f:\n",
    "    mwe_tokenizer = pickle.load(f)\n",
    "\n",
    "def apply_mwe_to_tokens(row):\n",
    "    tokens = mwe_tokenizer.tokenize(row)\n",
    "    cleaned_tokens = [token.replace('\\t', '') for token in tokens if not token.isnumeric()]\n",
    "    return cleaned_tokens\n",
    "\n",
    "df_test[\"cleaned_tokens\"] = df_test[\"cleaned_tokens\"].apply(apply_mwe_to_tokens)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<36x5926 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1898 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting to tfidf vectors\n",
    "X_tfidf = tfidf_vectorizer.transform(df_test['cleaned_tokens'].apply(lambda x: \" \".join(x)))\n",
    "numerical_features = df_test.drop(columns=[\"Author\", \"cleaned_tokens\", \"Text\"])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "numerical_features = numerical_features.apply(pd.to_numeric, errors='coerce')\n",
    "numerical_features = numerical_features.fillna(0)\n",
    "numerical_features_scaled = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# preparing the x_test and y_test data\n",
    "X_test = hstack([X_tfidf, numerical_features_scaled])\n",
    "y_test = df_test['Author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('              precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '          AM       1.00      0.67      0.80         6\\n'\n",
      " '          CD       0.93      1.00      0.97        14\\n'\n",
      " '          DM       1.00      1.00      1.00         8\\n'\n",
      " '          DO       1.00      0.50      0.67         2\\n'\n",
      " '          FE       1.00      0.50      0.67         2\\n'\n",
      " '          TK       0.57      1.00      0.73         4\\n'\n",
      " '\\n'\n",
      " '    accuracy                           0.89        36\\n'\n",
      " '   macro avg       0.92      0.78      0.80        36\\n'\n",
      " 'weighted avg       0.93      0.89      0.89        36\\n')\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "\n",
    "pprint(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
