{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e561ff3-d678-4ba1-b056-070a95b0c423",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Importing Packages and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbca207b-7db4-4cb4-aed3-a99782a41af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install xgboost\n",
    "# %pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2f5ee9-5d80-4e41-9d42-9476545cc4c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "warnings.simplefilter(action='ignore')\n",
    "# warnings.resetwarnings() to reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "758dfaaa-f101-47e0-b927-6d37e913abc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>type_to_token_ratio</th>\n",
       "      <th>punctuations_count</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>POS_Nouns</th>\n",
       "      <th>POS_Verbs</th>\n",
       "      <th>flesch_reading_score</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>stopwords_number</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scoring in PROC DISCRIM is as easy as validati...</td>\n",
       "      <td>AM</td>\n",
       "      <td>215</td>\n",
       "      <td>37</td>\n",
       "      <td>5.810811</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>5</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>58.99</td>\n",
       "      <td>10.33</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>['scoring', 'proc', 'discrim', 'easy', 'valida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the GLM procedure, you may have used LSMEAN...</td>\n",
       "      <td>AM</td>\n",
       "      <td>782</td>\n",
       "      <td>129</td>\n",
       "      <td>6.062016</td>\n",
       "      <td>0.661972</td>\n",
       "      <td>11</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>0.286822</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>52.80</td>\n",
       "      <td>11.39</td>\n",
       "      <td>47</td>\n",
       "      <td>59</td>\n",
       "      <td>['glm', 'procedure', 'may', 'used', 'lsmeans',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first problem, accuracy of the data file, ...</td>\n",
       "      <td>AM</td>\n",
       "      <td>990</td>\n",
       "      <td>159</td>\n",
       "      <td>6.226415</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>19</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.234177</td>\n",
       "      <td>0.158228</td>\n",
       "      <td>46.88</td>\n",
       "      <td>11.39</td>\n",
       "      <td>67</td>\n",
       "      <td>66</td>\n",
       "      <td>['first', 'problem', 'accuracy', 'data', 'file...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the homogeneity of covariance matrices assu...</td>\n",
       "      <td>AM</td>\n",
       "      <td>934</td>\n",
       "      <td>146</td>\n",
       "      <td>6.397260</td>\n",
       "      <td>0.604938</td>\n",
       "      <td>17</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>0.312925</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>44.44</td>\n",
       "      <td>11.98</td>\n",
       "      <td>49</td>\n",
       "      <td>69</td>\n",
       "      <td>['homogeneity', 'covariance', 'matrix', 'assum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With a CONTRAST statement, you specify L, in t...</td>\n",
       "      <td>AM</td>\n",
       "      <td>1490</td>\n",
       "      <td>247</td>\n",
       "      <td>6.032389</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>29</td>\n",
       "      <td>15.687500</td>\n",
       "      <td>0.282869</td>\n",
       "      <td>0.127490</td>\n",
       "      <td>55.54</td>\n",
       "      <td>9.15</td>\n",
       "      <td>63</td>\n",
       "      <td>122</td>\n",
       "      <td>['contrast', 'statement', 'specify', 'l', 'cas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Author  text_length  \\\n",
       "0  Scoring in PROC DISCRIM is as easy as validati...     AM          215   \n",
       "1  In the GLM procedure, you may have used LSMEAN...     AM          782   \n",
       "2  The first problem, accuracy of the data file, ...     AM          990   \n",
       "3  If the homogeneity of covariance matrices assu...     AM          934   \n",
       "4  With a CONTRAST statement, you specify L, in t...     AM         1490   \n",
       "\n",
       "   word_count  avg_word_length  type_to_token_ratio  punctuations_count  \\\n",
       "0          37         5.810811             0.840000                   5   \n",
       "1         129         6.062016             0.661972                  11   \n",
       "2         159         6.226415             0.712766                  19   \n",
       "3         146         6.397260             0.604938                  17   \n",
       "4         247         6.032389             0.488372                  29   \n",
       "\n",
       "   avg_sentence_length  POS_Nouns  POS_Verbs  flesch_reading_score  \\\n",
       "0            12.333333   0.305556   0.166667                 58.99   \n",
       "1            18.428571   0.286822   0.093023                 52.80   \n",
       "2            16.000000   0.234177   0.158228                 46.88   \n",
       "3            18.250000   0.312925   0.081633                 44.44   \n",
       "4            15.687500   0.282869   0.127490                 55.54   \n",
       "\n",
       "   gunning_fog_index  unique_word_count  stopwords_number  \\\n",
       "0              10.33                 21                12   \n",
       "1              11.39                 47                59   \n",
       "2              11.39                 67                66   \n",
       "3              11.98                 49                69   \n",
       "4               9.15                 63               122   \n",
       "\n",
       "                                      cleaned_tokens  \n",
       "0  ['scoring', 'proc', 'discrim', 'easy', 'valida...  \n",
       "1  ['glm', 'procedure', 'may', 'used', 'lsmeans',...  \n",
       "2  ['first', 'problem', 'accuracy', 'data', 'file...  \n",
       "3  ['homogeneity', 'covariance', 'matrix', 'assum...  \n",
       "4  ['contrast', 'statement', 'specify', 'l', 'cas...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"cleaned_data.csv\", index_col=0)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aff5ef-3a22-4d59-bb3e-3bc0738ca49c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Generating Text Embeddings\n",
    "- bag of words (count vectorizer)\n",
    "- tf-idf\n",
    "- word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b90b1f61-aa9d-40a0-8f8f-c253e0552550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df_train[['text_length', 'word_count', 'avg_word_length',\n",
    "        'type_to_token_ratio', 'punctuations_count', 'avg_sentence_length',\n",
    "        'POS_Nouns', 'POS_Verbs', 'flesch_reading_score', 'gunning_fog_index',\n",
    "        'unique_word_count', 'stopwords_number', 'cleaned_tokens']]\n",
    "y = df_train['Author']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2023) # doing a train test split 80% training 20% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849f88d5-64e6-41f6-81ee-5c860f6477fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3815745422.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 12\u001b[0;36m\u001b[0m\n\u001b[0;31m    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "X = df_train.drop(['Text', 'Author'], axis=1)\n",
    "y = df_train['Author']\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, tokenizer=lambda x: x, lowercase=False)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_train['cleaned_tokens']\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "X = pd.concat([X.drop('cleaned_tokens', axis=1), tfidf_df], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "feature_importances = clf.feature_importances_\n",
    "importances = pd.Series(feature_importances, index=X_train.columns)\n",
    "sorted_importances = importances.sort_values(ascending=False)\n",
    "sorted_importances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e988b8-261f-4452-972a-0c714d224a6a",
   "metadata": {},
   "source": [
    "### 1. Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef2396-7f5e-4746-8d6e-bea29cb4b0e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb06311-7592-4d5f-9f08-4ccc866e44f0",
   "metadata": {},
   "source": [
    "### 2. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc16bc-e360-4c23-b9d9-5f4bcd80667b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "928d1c07-888e-4da3-ac7f-c1ff17aa383a",
   "metadata": {},
   "source": [
    "### 3. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c7a7fa-fe6a-4396-a8e5-46e6bd9463f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae60e9d2-92a3-4bed-b7d2-826002544131",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Training Models\n",
    "Classical Models: \n",
    "- logistic regression\n",
    "- naive bayes\n",
    "- support vector machines (SVM)\n",
    "\n",
    "Tree Based: \n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "\n",
    "Boosting:\n",
    "- XGBOOST\n",
    "- Adaboost\n",
    "- Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37456bdb-125f-4952-9038-ea2e4d0212ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Model Comparison and Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
